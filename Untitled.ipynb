{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import requests\n",
    "import lxml\n",
    "from lxml.html.soupparser import fromstring\n",
    "from selenium.webdriver.support.ui import WebDriverWait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'house.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-d09075b19333>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[0mtitleLabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImageTk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPhotoImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"house.jpg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[0mpanel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[0mpanel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpady\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\PythonData\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   2889\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2890\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2891\u001b[1;33m         \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2892\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'house.jpg'"
     ]
    }
   ],
   "source": [
    "from tkinter import *\n",
    "from PIL import ImageTk, Image\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def search(event):\n",
    "  # Create list of search terms.\n",
    "  # Function zipcodes_list() creates a list of US zip codes that will be \n",
    "  # passed to the scraper. For example, st = zipcodes_list(['10', '11', '606'])  \n",
    "  # will yield every US zip code that begins with '10', begins with \"11\", or \n",
    "  # begins with \"606\" as a single list.\n",
    "  # I recommend using zip codes, as they seem to be the best option for catching\n",
    "  # as many house listings as possible. If you want to use search terms other \n",
    "  # than zip codes, simply skip running zipcodes_list() function below, and add \n",
    "  # a line of code to manually assign values to object st, for example:\n",
    "  # st = ['Chicago', 'New Haven, CT', '77005', 'Jacksonville, FL']\n",
    "  # Keep in mind that, for each search term, the number of listings scraped is \n",
    "  # capped at 520, so in using a search term like \"Chicago\" the scraper would \n",
    "  # end up missing most of the results.\n",
    "  # Param st_items can be either a list of zipcode strings, or a single zipcode \n",
    "  # string.\n",
    "  global list_of_zipcodes\n",
    "\n",
    "  if len(list_of_zipcodes) == 0:\n",
    "    print(\"No inputs given\")\n",
    "    zipcodes_label['text'] = \"No Inputs Given\"\n",
    "    return\n",
    "\n",
    "  st = zl.zipcodes_list(st_items = list(list_of_zipcodes))\n",
    "\n",
    "  # Initialize the webdriver.\n",
    "  driver = zl.init_driver(\"/Users/jasontu/Projects/Real_Estate_Aggregator/Zillow/chromedriver\")\n",
    "\n",
    "  # Go to www.zillow.com/homes\n",
    "  zl.navigate_to_website(driver, \"http://www.zillow.com/homes\")\n",
    "\n",
    "  # Click the \"buy\" button.\n",
    "  zl.click_buy_button(driver)\n",
    "\n",
    "  # Create 11 variables from the scrapped HTML data.\n",
    "  # These variables will make up the final output dataframe.\n",
    "  df = pd.DataFrame({'address' : [], \n",
    "                     'bathrooms' : [], \n",
    "                     'bedrooms' : [], \n",
    "                     'city' : [], \n",
    "                     'days_on_zillow' : [], \n",
    "                     'price' : [], \n",
    "                     'sale_type' : [], \n",
    "                     'state' : [], \n",
    "                     'sqft' : [], \n",
    "                     'url' : [], \n",
    "                     'zip' : []})\n",
    "\n",
    "  # Get total number of search terms.\n",
    "  num_search_terms = len(st)\n",
    "\n",
    "  # Start the scraping.\n",
    "  for k in range(num_search_terms):\n",
    "      # Define search term (must be str object).\n",
    "      search_term = st[k]\n",
    "\n",
    "      # Enter search term and execute search.\n",
    "      if zl.enter_search_term(driver, search_term):\n",
    "          print(\"Entering search term number \" + str(k+1) +  \": '\" + search_term + \"' \" + \n",
    "                \" out of \" + str(num_search_terms))\n",
    "\n",
    "      else:\n",
    "          print(\"Search term \" + str(k+1) +  \": '\" + search_term + \"' \" + \n",
    "                \" failed, moving onto next search term\\n***\")\n",
    "          continue\n",
    "      \n",
    "      # Check to see if any results were returned from the search.\n",
    "      # If there were none, move onto the next search.\n",
    "      if zl.results_test(driver):\n",
    "          print(\"Search \" + str(search_term) + \n",
    "                \" returned zero results. Moving onto the next search\\n***\")\n",
    "          continue\n",
    "      \n",
    "      # Pull the html for each page of search results. Zillow caps results at \n",
    "      # 20 pages, each page can contain 26 home listings, thus the cap on home \n",
    "      # listings per search is 520.\n",
    "      raw_data = zl.get_html(driver)\n",
    "      print(str(len(raw_data)) + \" pages of listings found\")\n",
    "      \n",
    "      # Take the extracted HTML and split it up by individual home listings.\n",
    "      listings = zl.get_listings(raw_data)\n",
    "      \n",
    "      # For each home listing, extract the 11 variables that will populate that \n",
    "      # specific observation within the output dataframe.\n",
    "      for n in range(len(listings)):\n",
    "          soup = BeautifulSoup(listings[n], \"lxml\")\n",
    "          new_obs = []\n",
    "          \n",
    "          # List that contains number of beds, baths, and total sqft (and \n",
    "          # sometimes price as well).\n",
    "          card_info = zl.get_card_info(soup)\n",
    "          \n",
    "          # Street Address\n",
    "          new_obs.append(zl.get_street_address(soup))\n",
    "          \n",
    "          # Bathrooms\n",
    "          new_obs.append(zl.get_bathrooms(card_info))\n",
    "          \n",
    "          # Bedrooms\n",
    "          new_obs.append(zl.get_bedrooms(card_info))\n",
    "          \n",
    "          # City\n",
    "          new_obs.append(zl.get_city(soup))\n",
    "          \n",
    "          # Days on the Market/Zillow\n",
    "          new_obs.append(zl.get_days_on_market(soup))\n",
    "          \n",
    "          # Price\n",
    "          new_obs.append(zl.get_price(soup, card_info))\n",
    "          \n",
    "          # Sale Type (House for Sale, New Construction, Foreclosure, etc.)\n",
    "          new_obs.append(zl.get_sale_type(soup))\n",
    "          \n",
    "          # Sqft\n",
    "          new_obs.append(zl.get_sqft(card_info))\n",
    "          \n",
    "          # State\n",
    "          new_obs.append(zl.get_state(soup))\n",
    "          \n",
    "          # URL for each house listing\n",
    "          new_obs.append(zl.get_url(soup))\n",
    "          \n",
    "          # Zipcode\n",
    "          new_obs.append(zl.get_zipcode(soup))\n",
    "      \n",
    "          # Append new_obs to df as a new observation\n",
    "          if len(new_obs) == len(df.columns):\n",
    "              df.loc[len(df.index)] = new_obs\n",
    "\n",
    "  # Close the webdriver connection.\n",
    "  zl.close_connection(driver)\n",
    "\n",
    "  # Write df to CSV.\n",
    "  columns = ['address', 'city', 'state', 'zip', 'price', 'sqft', 'bedrooms', \n",
    "             'bathrooms', 'days_on_zillow', 'sale_type', 'url']\n",
    "  df = df[columns]\n",
    "  dt = time.strftime(\"%Y-%m-%d\") + \"_\" + time.strftime(\"%H%M%S\")\n",
    "  file_name = str(dt) + \".csv\"\n",
    "  df.to_csv(file_name, index = False)\n",
    "\n",
    "  zipcodes_label['text'] = \"Scraping Complete. Review the following CSV file: \\n\" + str(dt) + \".csv\"\n",
    "  return \n",
    "\n",
    "list_of_zipcodes = set()\n",
    "\n",
    "def update_inputs(self):\n",
    "  new_zip = entry.get()\n",
    "  if 3 <= len(new_zip) <= 5:\n",
    "    list_of_zipcodes.add(new_zip)\n",
    "  else:\n",
    "    return \n",
    "  output = \"Search the following zip codes: \"\n",
    "  for i in list_of_zipcodes:\n",
    "    output += i + \", \"\n",
    "  zipcodes_label['text'] = output\n",
    "  \n",
    "\n",
    "# GUI Stuff\n",
    "root = Tk()\n",
    "root.geometry('{}x{}'.format(400, 600))\n",
    "\n",
    "root.title('Zillow Scraper')\n",
    "\n",
    "titleLabel = Label(root, text=\"ZILLOW SCRAPER\", bg=\"white\", fg=\"black\", font = \"Verdana 16 bold\")\n",
    "titleLabel.pack()\n",
    "\n",
    "img = ImageTk.PhotoImage(Image.open(\"house.jpg\"))\n",
    "panel = Label(root, image = img)\n",
    "panel.pack(pady=(10,0))\n",
    "\n",
    "zipcodes_label = Label(root, text=\"Add a Zipcode to Begin!\", bg=\"white\", fg=\"black\", width=100, height=10, borderwidth=2, relief=\"groove\")\n",
    "zipcodes_label.pack(side=TOP,padx=10,pady=10)\n",
    "\n",
    "text1 = Label(root, text=\"Add a Zipcode to Search: \", bg=\"white\", fg=\"black\")\n",
    "text1.pack()\n",
    "\n",
    "entry = Entry(root, width=10)\n",
    "entry.pack(side=TOP,padx=10,pady=10)\n",
    "\n",
    "add_zipcode_button = Button(root, text=\"Add Zipcode\")\n",
    "add_zipcode_button.bind(\"<Button-1>\", update_inputs)\n",
    "add_zipcode_button.pack(padx=10,pady=(0,50))\n",
    "\n",
    "begin_button = Button(root, text=\"Find Properties\")\n",
    "begin_button.bind(\"<Button-1>\", search)\n",
    "begin_button.pack()\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
